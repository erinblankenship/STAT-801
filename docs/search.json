[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 801A Notes",
    "section": "",
    "text": "Course Goals for STAT 801A\nSTAT 801A is an introduction to research methods, and how statistical methods may be used to answer research questions. By the end of the course, you will:\n\nunderstand the role statistics plays in the research process, and how a statistical investigation works.\nunderstand statistical evidence, and what conclusions may be drawn based on the evidence and study design.\nbe able to make simple probability calculations, and be able to differentiate a few different probability distributions based on the scenario.\nunderstand that variablility is natural, and commonly used statistics such as the mean, variance, and others have their own probability distributions. Such a probability distribution is called a sampling distribution.\nunderstand the underlying logic behind commonly used statistical inference techniques (hypothesis tests and confidence intervals).\nrealize that the most appropriate statistical inference method changes based on the explanatory variable(s), response variable, and goals of the study.\nbe able to calculate and interpret statistical analyses for studies in which there is one (or fewer) explanatory variables.\nbe able to sketch a skeleton ANOVA table from a description of the study.\nuse statistical software appropriately.\nbe able to clearly write up the results of an analysis.",
    "crumbs": [
      "Course Goals for STAT 801A"
    ]
  },
  {
    "objectID": "Section 1 Introduction.html",
    "href": "Section 1 Introduction.html",
    "title": "1  Introduction to Data and the Scientific Method",
    "section": "",
    "text": "1.1 Step 1: Ask a research question\nSound scientific conclusions require evidence from data. Statistics is the science of collecting, analyzing, and drawing conclusions from data. The goal of STAT 801A is to introduce you to the statistical methods used to answer research questions.\nThe scientific method has been used for hundreds of years for discovering new knowledge, and can be summarized with the following diagram:\nIt’s not coincidental that the steps in the scientific method are closely related to the steps in a statistical investigation. These steps appear in Tintle et al. (2021), but are not at all unique to this textbook.\nHow do you think the steps in a statistical investigation map to the scientific method? Can you map the baby study to either paradigm?\nEach of these steps has a lot of moving parts, so we’ll look at each step in more detail and introduce some concepts and introductory definitions as we do so.\nStep 1 boils down to\nThis may involve\nLet’s consider the baby example.\nWhy is a well-stated research question so important?\nAsking a research question is often the hardest part of the process, and requires technical information and experience in the discipline. A big reason why you are in graduate school is to gain this information and experience! A statistician can help you narrow your research question and state it precisely, but will not be able to formulate it for you.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data and the Scientific Method</span>"
    ]
  },
  {
    "objectID": "Section 1 Introduction.html#step-2-design-a-study-and-collect-data",
    "href": "Section 1 Introduction.html#step-2-design-a-study-and-collect-data",
    "title": "1  Introduction to Data and the Scientific Method",
    "section": "1.2 Step 2: Design a study and collect data",
    "text": "1.2 Step 2: Design a study and collect data\nStep 2 involves\n\n\n\n\n\nThere is so much going on here that is not evident from the simple statement of “collect data.” Let’s first think about why there are so many things to consider.\nLet’s think about the babies. What questions do you think the researchers had to address in their design and data collection?\n\n\n\nWe’re trying answer a research question, and let’s specifically think about evaluating hypotheses (though the same applies to estimating an unknown quantity). We can almost never absolutely accept or reject a research theory for two reasons:\n\nVariability of experimental material\n\n\n\n\n\n\n\nSampling\n\n\n\n\n\n\nVariability and sampling are probably the two most important ideas in statistics, but they are also some of the hardest to grasp. Let’s lay out some basic concepts.\nA researcher’s major goal is to make general statements about their question as it applies to their population of interest.\n\n\n\n\n\n\n\nPopulations can be finite or infinite. Even if the population is finite, we typically can’t measure all of the units in the population. So, to collect data, we must select a subset of the population, a sample and hope that the subset is representative of the population.\nWe’d really rather not rely on hope, and collect data in a way that ensures the sample represents the population. This is typically accomplished by random sampling\n\n\n\n\n\n\n\n\nThere are other considerations as well, typically driven by both the research question and practicality. These include:\nExperiment or observational study?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf it’s an experiment, what is the experimental unit?\n\n\n\n\n\n\n\n\n\n\n\n\nWhat variable(s) will be measured?\n\n\n\n\n\n\n\n\n\n\n\n\nHow will the variables be measured? With how much precision?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf two or more variables are measured, can one be considered the response variable and the other(s) be considered explanatory?\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs it possible to employ random sampling, random assignment, or both?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow many observations should we collect?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data and the Scientific Method</span>"
    ]
  },
  {
    "objectID": "Section 1 Introduction.html#step-3-explore-the-data",
    "href": "Section 1 Introduction.html#step-3-explore-the-data",
    "title": "1  Introduction to Data and the Scientific Method",
    "section": "1.3 Step 3: Explore the data",
    "text": "1.3 Step 3: Explore the data\nExploring the data means\n\n\n\n\n\n\nFor example, consider the histogram below. It shows the percent of residents aged 65 years and over in the 50 US states and District of Columbia.\n\n\n\nHistogram of percent of residents aged 65 and over.\n\n\nDo you think these outliers are the result of a recording error?\nHowever, exploring the data goes beyond looking for unexpected outcomes, it also encompasses exploratory data analysis (EDA). EDA includes both numerical exploration and graphical exploration. Our textbook does a great job summarizing both numerical and graphical summaries of data (pages 30-73), including walking through how EDA can be used in several case studies.\nWe won’t spend a lot of time here, since these are mostly very familiar concepts (mean, median, etc.) However, we’ll go through a small example as a preview of coming attractions.\nExample: The Gettysburg Address is comprised of 268 words, with word lengths varying from 1 (“a”) to 11 (“consecrated”) letters. Supposed we’re interested in the average word length.\nThe population of interest is\n\n\n\nWe’re going to take a random sample of \\(n=9\\) words. The sample is\n\nRandom sample of 9 words from the Gettysburg Address\n\n\nWord ID\nWord\nLength\n\n\n\n\n53\nlong\n4\n\n\n31\nNow\n3\n\n\n120\nbrave\n5\n\n\n263\nshall\n5\n\n\n264\nnot\n3\n\n\n249\nof\n2\n\n\n221\nfull\n4\n\n\n144\nnote\n4\n\n\n209\ntake\n4\n\n\n\nUsing our sample, we can easily find the sample mean and sample median.\n\n\n\n\n\nThese values are statistics.\n\n\n\n\nWe typically use statistics to estimate parameters.\nIn this case, we can actually calculate the parameters, because we have access to the entire population.\n\n\n\n\nThis is a very artificial situation. Most of the time, we only have the data in the sample and we want to use the statistics to make some statements about the parameters.\nWe may also be interested in how much variability there is among word lengths. There are a few ways we could quantify variability. Again, let’s consider the sample of 9 words.\n\n\n\n\nAgain, these are statistics because they’re calculated only from our sample of \\(n=9\\) observed words. In this case, we can get the parameters.\n\n\n\n\n\n\nIf we were to take a different sample of size \\(n=9\\), we’d likely get different statistics. Let’s try it.\n\n\n\n\n\n\nSo the sample mean, a statistic, is itself a random variable. There is uncertainty associated the outcome.\nWhat happens if we draw samples that are bigger than \\(n=9\\)?\n\n\n\n\n\n\nSo, the sample mean has its own variance, which depends on the sample size. Specifically,\n\n\n\n\n\n\n\nBut, in real life, we only observe one sample–which means we get one mean and one variance. We need to understand the underlying behavior/variability of the sample statistics to be able to use them to make statements about the population parameters. This is why variability and sampling are such important concepts in statistics. We need to know how our sample statistic behaves in order to …",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data and the Scientific Method</span>"
    ]
  },
  {
    "objectID": "Section 1 Introduction.html#step-4-draw-inferences-beyond-the-data",
    "href": "Section 1 Introduction.html#step-4-draw-inferences-beyond-the-data",
    "title": "1  Introduction to Data and the Scientific Method",
    "section": "1.4 Step 4: Draw inferences beyond the data",
    "text": "1.4 Step 4: Draw inferences beyond the data\nThe general idea in drawing inferences beyond the data\n\n\n\n\n\nBasically, we’re trying to see what the sample data tells us about the population of interest.\n\n\n\n\n\n\nLet’s go back to the babies. If the babies really can’t tell right from wrong, how likely is a baby to pick the good character?\n\n\n\n\nWe haven’t even seen the data yet, but we can think about how a sample statistic should behave. What was measured? What is the sample statistic of interest? Once we get a handle on how the sample statistic should behave, we can assess how unusual the observed data actually are, if the babies really can’t tell right from wrong.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data and the Scientific Method</span>"
    ]
  },
  {
    "objectID": "Section 1 Introduction.html#step-5-formulate-conclusions",
    "href": "Section 1 Introduction.html#step-5-formulate-conclusions",
    "title": "1  Introduction to Data and the Scientific Method",
    "section": "1.5 Step 5: Formulate conclusions",
    "text": "1.5 Step 5: Formulate conclusions\nHere, our conclusions must consider the scope of inference made in Step 4.\n\n\n\n\n\n\nIt’s important to keep in mind the population of interest, and whether we employed random assignment, random sampling, both, or neither.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data and the Scientific Method</span>"
    ]
  },
  {
    "objectID": "Section 1 Introduction.html#step-6-look-back-and-ahead",
    "href": "Section 1 Introduction.html#step-6-look-back-and-ahead",
    "title": "1  Introduction to Data and the Scientific Method",
    "section": "1.6 Step 6: Look back and ahead",
    "text": "1.6 Step 6: Look back and ahead\nThis step involves\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we progress through the semester, Step 4 is where we’ll spend most of our time. We’ll consider different types of variables, different research goals, different study designs, and how we can use the data to draw inferences to a larger population.\nAs we saw earlier, in order to draw those inferences we need to understand and be able to quantify how much variability we expect to see in the sample statistic. We also need more precise definitions and rules around the uncertainty associated with data. In the next section, we’ll discuss the basics of probability and probability distributions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data and the Scientific Method</span>"
    ]
  },
  {
    "objectID": "Section 2 Probability.html",
    "href": "Section 2 Probability.html",
    "title": "2  Probability Basics and Probability Distributions",
    "section": "",
    "text": "2.1 Probability Basics\nProbability is the language we use to talk about chance and quantify uncertainty. A probability is a number between 0 and 1, where an event is more likely the closer the probability is to 1.\nWe’ve already seen a probability! Back to the babies–when we considered how unusual it was to see 13/16 babies pick the good puppet, we calculated:\nThe value we calculated is a p-value: the (empirical) probability of observing what we did in the data (or something even more extreme), under the assumption that the null hypothesis is true. For better or worse, science runs on p-values.\nIn this section, we’ll see some basic probability theory and calculations, as well as probability distributions.\nWhen we are uncertain about an outcome’s occurrence (e.g., whether a coin will come up heads or tails, the number of dots observed on the roll of a die, whether or not the bus will be late), we typically quantify this uncertainty with a probability. Probability is the foundation upon which all of statistics is built, and it a provides a framework for modeling populations, experiments, and almost anything that could be considered a random phenomenon.\nA sample space, denoted by \\(S\\), is comprised of all possible outcomes of a random phenomenon.\nAn event is a collection of possible outcomes. Each event \\(A\\) is a subset of \\(S\\).\nWe want to formalize the idea of the “chance” that event \\(A\\) occurs. We will do this by defining the probability of each \\(A\\), which we denote \\(P(A)\\).\nProbabilities are calculated by defining functions on sets, and should be defined for all possible events. One thing that must be true: \\[\n0 \\leq P(A) \\leq 1\n\\]\nMore formally, a probability function is defined as follows.\nGiven a sample space \\(S\\), a probability function is a function P(\\(\\cdot\\)) that satisfies\nAny function P(\\(\\cdot\\)) that satisfies these three requirements is called a probability function.\nIf we let \\(S\\) be a sample space with associated probability function P, we can state some basic facts. Let \\(A, B\\) be events in \\(S\\).\nWe’ll use these facts when calculating probabilities. First, however, we need to figure out how to assign probabilities to specific events. There are several ways we can do this.\nHowever we arrive at probabilities for a given scenario, we can use them to construct a probability distribution. There are several flavors of probability distribution. The simplest is a list of all possible outcomes and their associated probabilities, and it must satisfy three rules:\nAny probability distribution that can be written this way corresponds to a discrete variable or one that we have discretized.\nWe’ll see some other (more common, but more complicated) flavors of probability distributions in a bit, after some facts and definitions.\nConsider the following table:\nThe counts in the table are the number of Titanic passengers that fell into each of the categories. From this table, we can calculate some probabilities.\nSometimes we have partial information about a certain event and wish to know how this affects the probablities of other events, if at all. For example, we might be interested in the probability a passenger survived, given they were in First Class. This is called conditional probability.\nDefinition:\nExample: Toss a fair die. Let \\(A=\\{1\\}\\) and let \\(B=\\{1,3,5\\}\\). What is the probability of throwing a 1, given an odd number was thrown?\nThis definition of conditional probability leads to:\nLet \\(A_1, A_2, \\dots\\) be a collection of mutually exclusive and exhaustive events. What does this mean?\nSuppose we want the probability of an event \\(B\\).\nThis leads to the general form of Bayes’ Theorem:\nExample: (Problem 2.18) A genetic test is used to determine if people have a predisposition for thrombosis, which is a formation of a blood clot inside a blood vessel that obstructs the flow of blood through the circulatory system. It is believed that 3% of people actually have this predisposition. The genetic test is 99% accurate if a person actually has the predisposition. The test is 98% accurate if a person does not have the predisposition.\nWhat is the probability a randomly selected person who tests positive for the predisposition by the test actually has the predisposition?\nConsider the following table, which summarizes all flights arriving at an airport in a single day:\nWhat is the probability a randomly selected flight on this day was on time?\nWhat is the probability a randomly selected flight was on time, given it was a domestic flight?\nWhat do you notice?\nDoes this make sense in the context of this scenario? What do you think it means?\nSometimes the occurrence of one event, \\(B\\), will have no effect on the probability of another event, \\(A\\). If \\(A\\) and \\(B\\) are unrelated, then intuitively it should be the case\nAlso, it follows that\nDefinition:\nHow is independence used? Let’s do a pretty famous example. We’ll use a few of the rules we’ve seen so far.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Basics and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Section 2 Probability.html#probability-basics",
    "href": "Section 2 Probability.html#probability-basics",
    "title": "2  Probability Basics and Probability Distributions",
    "section": "",
    "text": "Equally likely outcomes\n\n\n\nRelative frequencies\n\n\n\nMaking assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvived\nDid Not Survive\n\n\n\n\nFirst Class\n201\n123\n\n\nSecond Class\n118\n166\n\n\nThird Class\n181\n528\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLate\nOn Time\n\n\n\n\nDomestic\n12\n109\n\n\nInternational\n6\n53",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Basics and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Section 2 Probability.html#random-variables-and-probability-distributions",
    "href": "Section 2 Probability.html#random-variables-and-probability-distributions",
    "title": "2  Probability Basics and Probability Distributions",
    "section": "2.2 Random Variables and Probability Distributions",
    "text": "2.2 Random Variables and Probability Distributions\nTypically we are interested in a numerical measurement of the outcome of a random experiment. For example, we might want to know the number of insects treated with a dose of a new insecticide that are killed. In this case, the outcome is the survival status of each dosed insect and the numerical measurement we’re interested in is the number that died. However, the observed number varies depending on the actual result of the experiment. This type of variable is called a random variable.\nDefinition: A random variable is a function that associates a real number with each element in the sample space. That is, a random variable is a function from a sample space, \\(S\\), into the real numbers.\nExample: Suppose we roll two dice and we’re interested in the number of 1s that are thrown.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom variables can also be defined on a continuous range.\nExample: Take a 1 gram soil sample and measure the amount of phosphorus in the sample (in g).\n\n\n\n\nWe’ve already seen one flavor of probability distribution: a list of possible outcomes for the random variable, and the associated probabilities.\n\n\n\n\n\nWe can define probability distribution more generally.\nDefinition: A probability distribution is a function that is used to assign probability to each value the random variable can take on.\nMaybe that function can be written in tabular form, as above, maybe it’s a function in the mathematical function sense (we’ll see some of these later in this section). We can have probability distributions for discrete random variables and continuous random variables.\nDiscrete probability distributions\n\nProbabilities are denoted P\\((X=x)\\) for the realized value \\(x\\) of random variable \\(X\\)\n\\(\\sum_i\\) P\\((X=x_i)\\) = 1.\n\n\n\n\n\nExample: We have two seeds in a Petri dish, and will observe how many germinate. We assume the seeds germinate independently, and the probability a randomly selected seed germinates is 0.80.\n\n\n\n\n\n\nContinuous probability distributions\n\nThis distribution is called a probability density function (pdf) and denoted \\(f(x)\\).\nThe area bounded by \\(f(x)\\), the horizontal axis, and the values \\(a\\) and \\(b\\) is P\\((a \\leq X \\leq b)\\).\nThe total area under the pdf is 1.\n\n\n\n\n\nExample: Let \\(X\\) = phosphorus in a 1 gram soil sample. Suppose we assume the pdf is \\[\nf(x) = \\left \\{ \\begin{array}{ll} 1 & 0 \\leq x \\leq 1 \\\\ 0 & x &lt; 0, x &gt; 1 \\end{array} \\right .\n\\]\n\n\n\n\n\n\n\n\nJoint probability distributions: We’ve already seen some of these! A joint probability distribution can be used to study the relationship between two variables, \\(X\\) and \\(Y\\), simultaneously. We’re going to restrict our attention to discrete joint probability distributions, and summarize them as two-way tables.\nLet’s go back to the Titanic example:\n\n\n\n\nSurvived\nDid Not Survive\n\n\n\n\nFirst Class\n201\n123\n\n\nSecond Class\n118\n166\n\n\nThird Class\n181\n528\n\n\n\n\n\n\n\n\nIf we know the probability distribution for a random variable, we can use it to calculate things like the “true” mean and variance for that variable.\nExpected value: The expected value (or mean) of a discrete random variable is defined as\n\n\n\n\n\n\n\n\n\nThere are some rules that come along with expected values (discrete or continuous):\n\nIf \\(X\\) is a random variable and \\(c\\) is a constant, then\n\n\n\n\n\n\nIf \\(X\\) is a random variable, \\(b\\) and \\(c\\) are constants, and \\(Y=bX + c\\), then\n\n\n\n\n\n\nIf \\(X\\) and \\(Y\\) are random variables, \\(b\\) and \\(c\\) are constants, and \\(W=bX + cY\\), then\n\n\n\n\n\n\n\n\nExample: Let \\(X\\) = number of 1s thrown when rolling two dice.\n\n\n\n\nVariance: The variance of a discrete random variable is defined as\n\n\n\n\n\n\n\n\nThere are also rules that come along with variance (discrete or continuous):\n\nFor any random variable \\(X\\) and any constant \\(c\\),\n\nIf \\(X\\) is a random variable, \\(b\\) and \\(c\\) are constants, and \\(Y=bX + c\\), then\n\n\n\n\n\nIf \\(X\\) and \\(Y\\) are independent random variables, and \\(b\\) and \\(c\\) are constants, then\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf \\(X\\) and \\(Y\\) are any two random variables, and \\(b\\) and \\(c\\) are constants, then\n\n\n\n\n\n\n\n\n\n\n\nExample: In Mendel’s experiments on pea plants, he found the trait of being tall is dominant over being short. His theory indicates that if pure-line tall and pure-line short plants are cross-pollinated and then the hybrids in the next generation are cross-pollinated, in the resulting population approximately 3/4 of the plants will appear tall and 1/4 will appear short. If four plants are chosen at random from such a population, the best model (i.e., probability distribution) for the number of tall plants out of the four is\n\n\n\n\\(y\\)\n0\n1\n2\n3\n4\n\n\n\n\nP\\((Y=y)\\)\n1/256\n12/256\n54/256\n108/256\n81/256\n\n\n\n\nFind the expected number of tall plants\n\n\n\n\n\n\n\n\n\nFind the variance of number of tall plants\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind the standard deviation of number of tall plants\n\n\n\n\n\nWhat is the probability that the value of \\(Y\\) will be more than 2 standard deviations below the expected value?\n\n\n\n\nExample: Three patients receive injections to desensitize them from an allergen. The serum used is said to be 90% effective. Let \\(X\\) denote the number of patients who become desensitized.\n\nFind the probability distribution of \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind the expected number of patients that will become desensitized.\n\n\n\n\n\n\n\n\n\nFind the variance and standard deviation of the number of patients who become desensitized.\n\n\n\n\n\n\n\n\n\nIf a patient does not become desensitized, the insurance company will spend $50 on additional treatment. How much should the insurance company expect to pay in additional costs for these three patients?\n\n\n\n\n\nExample: A forester is studying a population of trees that are known to have a mean height of 23.4 ft with a variance of 256 ft\\(^2\\). A tree is randomly selected from the population and its height is measured in feet. Let \\(X\\) represent the height of the randomly selected tree.\n\nWhat is the selected tree’s expected height in meters? (there are 0.3048 meters in a foot)\n\n\n\n\n\n\nWhat is the variance of the height of the selected tree in meters?\n\n\n\n\nExample: Contracts for two construction jobs are randomly assigned to one or more of three firms: A, B, and C. Let \\(Y_1\\) denote the number of contracts assigned to firm A and \\(Y_2\\) the number of contracts assigned to firm B. The joint probability distribution for this scenario is\n\n\n\n\n\n\n\n\n\nFind the expected number of contracts awarded to Firm A.\n\n\n\n\n\nFind the expected number of contracts awarded to Firm B.\n\n\n\n\n\nFind the variance of number of contracts awarded to Firm A.\n\n\n\n\n\nFind the variance of number of number of contracts awarded to Firm B.\n\n\n\n\nFind the expected number of contracts awarded to either Firm A or Firm B.\n\n\n\n\nFind the variance of the number of contracts award to either Firm A or Firm B.\n\n\n\n\n\nWhat now? What is this Cov?\nCovariance is a measure of the linear relationship between two random variables. It can be positive or negative. A positive covariance indicates that as the value of one RV increases, so does the other. A negative covariance indicates that as the value of RV increases, the other decreases.\nFor discrete RVs, the covariance is calculated as\n\n\n\n\n\nIf two random variables are independent, the covariance is 0.\n\n\nFor our example, do you think covariance will be positive, negative, or 0?\nLet’s calculate it, and find the variance above.\n\n\n\n\n\n\n\n\n\n\nNote the units of measurement on covariance.\n\n\n\n\n\nThis makes covariance less intuitive as a measure of dependence–its value depends on the scale of measurement. A measure of dependence that is not dependent on scale is the correlation:\n\n\n\n\n\nThe correlation is unitless, and must be \\(-1 \\leq \\rho \\leq 1\\). Just like covariance, if two random variables are independent, their correlation will be 0.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Basics and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Section 2 Probability.html#special-probability-distributions",
    "href": "Section 2 Probability.html#special-probability-distributions",
    "title": "2  Probability Basics and Probability Distributions",
    "section": "2.3 Special Probability Distributions",
    "text": "2.3 Special Probability Distributions\nEarlier, we mentioned that some probability distributions can be written as mathematical functions. We’re going to discuss some probability distributions that commonly arise in data analysis.\n\n2.3.1 The Binomial Distribution\nIn some studies, the variable of interest only has two potential outcomes: success and failure. These could be died/survived, yes/no, occurred/did not occur, picked the good puppet/picked the bad puppet. Under some very specific conditions, variables like these follow a theoretical probability distribution called the binomial distribution.\nHere are the conditions we need:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf these conditions are met, the probability distribution of \\(X\\) = number of “successes” observed in \\(n\\) trials is\n\n\n\n\n\n\nIf \\(X\\) follows a binomial distribution with parameter \\(p\\), then\n\n\n\n\nRight now, we’ll use the binomial distribution to calculate some probabilities assuming a specific value for \\(p\\), but inference for scenarios like this typically focuses on testing hypotheses about \\(p\\) (like the babies!) and estimating \\(p\\).\nExample: A new variety of turfgrass has been developed for use on golf courses, with the goal of obtaining a germination rate of 85%. To evaluate the grass, 20 seeds are planted in a greenhouse so that each seed will be exposed to identical conditions. If the 85% germination rate is correct, what is the probability that 18 or more seeds will germinate?\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow many seeds do we expect to germinate? What is the variance of the number of germinated seeds?\n\n\n\n\n\n2.3.2 The Poisson Distribution\nThe Poisson distribution models count data, typically the number of events observed for a particular unit of time or space. For example, the Poisson can be used to model variables like:\n\nthe number of hits to a website per minute\nthe number of PCB particles in a liter of water\nthe number of insects in a square meter\nthe number of cars passing through an intersection in 5 minutes\nthe number of flaws in a yard of fabric\n\nLike the Binomial, the Poisson has some requirements:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe probability distribution for the Poisson is\n\n\n\n\n\n\n\nThe Poisson distribution has a couple of interesting features:\n\n\n\n\n\nExample: Suppose grasshoppers are distributed at random in a large field according to a Poisson distribution with \\(\\lambda=2\\) grasshoppers per square meter.\n\nFind the probability that no grasshoppers will be found in a randomly selected square meter.\n\n\n\n\n\n\n\n\n\nFind the probability that 2 or fewer grasshoppers will be found in 2 square meters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind the expected number of grasshoppers in 10 square meters.\n\n\n\n\n\n\nFind the expected number of grasshoppers in 0.5 square meters.\n\n\n\n\n\n\n2.3.3 The Normal Distribution\nThe most commonly used continuous distribution (maybe the most commonly used distribution, period) is the normal distribution. It’s commonly used because\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe normal distribution is bell-shaped, symmetric, and unimodal. In fact, we shouldn’t call it the normal distribution, there are an infinite number of different normal distributions, depending on the parameters of the distribution, \\(\\mu\\) and \\(\\sigma^2\\).\n\n\\(\\mu\\) represents the mean of the distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\sigma^2\\) represents the variance of the distribution\n\n\n\n\n\n\n\n\n\n\n\nThe normal distribution does has a mathematical function (a pdf) that governs its shape:\n\n\n\n\n\n\n\nWe denote random variables following the normal as\n\n\n\nand the normal with mean \\(\\mu=0\\) and variance \\(\\sigma^2=1\\) is called the standard normal distribution.\nThe standard normal gives us a convenient way to compare observations, and any normal distribution can be transformed into a standard normal. The Z-score is\n\n\n\n\n\n\nIf the Z-score is positive\n\n\nIf the Z-score is negative\n\n\nZ-scores can be used to\n\ngauge the unusualness of an observation\n\n\n\n\n\nfind probabilities\n\n\n\n\n\nSome helpful R functions:\n\npnorm(x, mean=0, sd=1)\nqnorm(prob, meam=0, sd=1)\n\n\n\n\nnormTail(m=0,s=1, L=x) or normTail(m=0,s=1,U=x) (does require the OpenIntro library)\n\nExample: Full-term birth weights for single babies are normally distributed with a mean of 7.5 pounds and a standard deviation of 1.1 pounds.\n\nA randomly selected newborn weighs 9.1 pounds. What is the weight percentile for this baby?\n\n\n\n\n\n\n\nBabies that weigh less than 5.5 pounds are considered low birth weight. What proportion of babies are low birth weight?\n\n\n\n\n\n\n\n\nWhat weight would make a baby at the 25th percentile?\n\n\n\n\n\n\n\n\nWhat is the probability a randomly selected baby weighs between 7 and 8 pounds?\n\n\n\n\n\n\nThe Empirical Rule (aka the 68-95-99.7 Rule) presents a general rule for the probability of falling within one, two, and three standard deviations of the mean in a normal distribution.\n\n\n\n\n\n\n\n\n\n\n\nThis rule is useful in a wide range of settings when trying to make a quick estimate.\nThe normal distribution is useful because it can be used to approximate other distributions, such as the binomial.\nLet’s see what happens with \\(p=0.15\\) as we change the sample size.\n\n\n\n\n\nRecall the binomial distribution has\n\n\nIf \\(n\\) is sufficiently large, the binomial can be well-approximated with a normal distibution with \\(\\mu=np\\) and \\(\\sigma^2 = np(1-p)\\).\nWhat’s sufficiently large?\n\n\nExample: (problem 3.33) Suppose a university announced that it admitted 2500 students for the incoming first year class. However, the univesity has dorm room spots for only 1786 first year students. If there is a 70% chance an admitted student will enroll at the university, what is the probability the university will not have enough dorm room spots?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Basics and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Section 3 Sampling Distributions.html",
    "href": "Section 3 Sampling Distributions.html",
    "title": "3  Sampling Distributions and Foundations of Statistical Inference",
    "section": "",
    "text": "3.1 Sampling Distributions\nAs we’ve seen in the last two chapters, variability is natural and expected. We expect to see variability in observations, which implies there will also be variability in summary statistics. We’ve seen this already:\nIf we want to use a summary statistic (like \\(\\bar X\\) or \\(\\hat p\\)) calculated from our sample to draw inferences about the population, we have to understand how the summary statistic behaves.\nThis means, we need to know the sampling distribution of the statistic.\nAs a refresher, the goal of statistical inference is to use an observed data set to answer questions about the overall population from which the sample data set was drawn. Typically, those questions may be answered using some parameter(s) of the population distribution.\nA parameter is\nFor example,\nParameters are generally fixed, unknown constants. We want to use our sample data to answer a question about the parameter (hypothesis test) or estimate the parameter (confidence interval). We may also be interested in functions of parameters.\nOften, the statistic we’ll use to estimate the underlying parameter is pretty intuitive.\nBut, if we want to use a statistic, we have to understand its behavior.\nThe sampling distribution is\nWe’ve can study sampling distributions empirically, through simulation. We’ve already done this!\nWe can also quantify sampling distributions theoretically. We’ve already done this too!\nThe sampling distributions we’ve seen so far have been (mostly):\nThis isn’t coincidence it’s guaranteed by a very important theorem, the Central Limit Theorem.\nCentral Limit Theorem:\nBut wait, the sample mean? Weren’t we also considering sample proportions?\nLet’s think more about these requirements:\nIf the Central Limit Theorem holds, the underlying parameters of the resulting approximate normal distribution will depend on the population from which the original data were drawn.\nOther statistics will have sampling distributions that do not follow an approximate normal. For example, the sample variance is a natural estimate for the population variance. But, the CLT does not apply to variances. We’ll need a different distribution.\nOnce we can articulate the sampling distribution, we can use it to do statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Distributions and Foundations of Statistical Inference</span>"
    ]
  },
  {
    "objectID": "Section 3 Sampling Distributions.html#sampling-distributions",
    "href": "Section 3 Sampling Distributions.html#sampling-distributions",
    "title": "3  Sampling Distributions and Foundations of Statistical Inference",
    "section": "",
    "text": "Independence\n\n\n\n“Large enough”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Distributions and Foundations of Statistical Inference</span>"
    ]
  },
  {
    "objectID": "Section 3 Sampling Distributions.html#foundations-of-statistical-inference",
    "href": "Section 3 Sampling Distributions.html#foundations-of-statistical-inference",
    "title": "3  Sampling Distributions and Foundations of Statistical Inference",
    "section": "3.2 Foundations of Statistical Inference",
    "text": "3.2 Foundations of Statistical Inference\nIn Chapter One, we talked about framing a research question. Many (but not all) research questions can be answered using statistical inference. We’ll now lay out the basic logic of statistical inference, illustraing the different methods for the case in which we have a single response variable (quantitative or categorical) and no explanatory variable. The framework for statistical inference will not change as we move to more complicated scenarios.\nStatistical inference is a collection of techniques which use information from a sample to make precise statements about the entire population. In STAT 801A, the general statements about populations will be expressed in terms of the parameters, or functions of parameters, of probability distributions. Because we know the sampling distribution, we can use probability to precisely quantify the accuracy of our general statements.\nStatistical inference is broken into two broad categories: estimation and testing. These map back to the types of research questions we outlined in Chapter One.\n\n\n\n\n\n\n\n3.2.1 Estimation\nThis category of statistical inference is concerned with using sample information to estimate one or more parameters, or functions of parameters, of the probability distribution for a population. For example, we may be interested in estimating the mean of a population, or the difference in means between two populations. There are two types of estimation, point estimation and interval estimation.\nPoint estimation\n\n\n\n\n\nBut a single value is not very meaningful without some way of telling how close our estimate comes to the true value.\nInterval estimation\n\n\n\n\n\nWe’ll illustrate how interval estimation works with an example.\nExample: An entomologist is studying a new tick species that may be the carrier of the pathogen associated with lyme disease. They design a study to estimate prevalence of the pathogen in the tick. They examine 200 ticks randomly selected in the study region during a period of the year when ticks have been known to be infected with the pathogen in other regions of the country. They find 18 ticks that are infected with the pathogen.\n\nParameter of interest:\n\n\n\n\n\nSample statistic:\n\n\n\n\nAre the requirements for the Central Limit Theorem met?\n\n\n\n\nThe Central Limit Theorem tells us\n\n\n\n\n\n\nNow let’s consider the Empirical Rule.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost (but not all) confidence intervals have the form:\n\n\n\n\n\n\n\nSo, to calculate a confidence interval of this form we’ll need the margin of error, which is calculated based on the standard error of the statistic and the sampling distribution of the statistic. We’ll also need to specify how much certainly we want in our interval estimate.\n\n\n\n\nBack to the ticks.\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nWhat happens if we change our level of confidence?\n\n\n\n\n\n\n\n\n\nWhat if we want a confidence level that isn’t 68, 95, or 99.7?\n\n\n\n\n\n\n\n\n\n\n\nLet’s think more carefully about what this confidence level means. A confidence interval is a probability statement, but not the probability statement that is intuitive. Suppose we are interested in an interval estimate for a parameter \\(\\theta\\).\n\n\n\n\nIt’s super important to understand that this probability statement is only valid for as long as \\(L\\) and \\(U\\) are unknown. Once we use the data to estimate \\(L\\) and \\(U\\), and get \\(\\hat{L}\\) and \\(\\hat{U}\\), the interval is no longer random. The interval either contains the parameter or it doesn’t. This means statements like\n\n\n\n\n\nare incorrect, as tempting as they are to write. Rather, the statement of probability is about the method used to obtain the confidence interval.\nLet’s look at the applet to explore what that confidence level really means: Applet\n\n\n\n\n\n\n\n\nSo, let’s find a 98% confidence interval for the proportion of ticks that are infected by the pathogen.\n\n\n\n\n\n\n\n\n\n\n\n\nExample: (4.17, sort of) The nutrition label on a bag of potato chips says that a one ounce serving has 130 calories and 10 grams of fat. A random sample of 35 bags yielded a sample mean of \\(\\bar{x}=134\\) calories with a sample standard deviation of \\(s = 17\\) calories. Assume the distribution of bags is relatively symmetric. We want a 95% confidence interval for the true mean calorie count of a bag of potato chips.\nWhat’s different about this example, compared to the tick example?\n\n\n\n\nLet’s state the Central Limit Theorem again.\n\n\n\n\n\n\n\n\nThis presents a few complications:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe natural fix is to use \\(s\\) (the sample standard deviation) in place of \\(\\sigma\\), so the standard error is\n\n\n\n\nBut this leads to yet another complication: the normal distribution isn’t quite right. Instead, we end up with a distribution that has heavier tails than the normal. Instead, we use the \\(t\\) distribution. The \\(t\\) distribution has a single parameter, the degrees of freedom (\\(df\\)). The degrees of freedom determines the shape of the \\(t\\), with the distribution getting closer and closer to the normal as the \\(df\\) increase.\n\n\n\nStandard normal compared to the t distribution with various df\n\n\nIn the scenario of a single mean, \\(df=n-1\\) but this will change as the scenario gets more complicated.\nWe can get \\(t\\) probabilities and quantiles using the R functions\n\npt(x, df=)\nqt(prob, df=)\n\nSo, if we’re interested in calculating an interval estimate for a mean\n\n\n\nBack to the potato chips example. Are the conditions for the Central Limit Theorem met?\n\n\n\n\nWe’ll calculate a 95% confidence interval.\n\nExample: An ichthyologist is interested in estimating the variance of lengths of trout minnows in a very large tank at a fish hatchery. It is reasonable to assume that lengths are normally distributed. 15 minnows are randomly sampled from the tank and measured. The sample variance is \\(s^2 = 0.17\\) inch\\(^2\\).\nWhat’s different now?\n\n\n\n\n\nWhat complications does this present?\n\n\n\n\n\n\n\n\n\n\n\nWe need a new distribution! We need the sampling distribution of \\(S^2\\). It turns out that a function of \\(S^2\\) follows the \\(\\chi^2\\) distribution. The \\(\\chi^2\\) has the following properties:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf our original observations come from a normal distribution, then\n\n\n\nThis gives us a straightforward way to find a confidence interval for \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can find these \\(\\chi^2_{\\alpha/2}\\) and \\(\\chi^2_{1-\\alpha/2}\\) using the qchisq(prob,df=) function in R. For example,\n\n\n\n\n\n\n\n\n\n\nThis is one of the cases where the confidence interval does not have the estimate \\(\\pm\\) margin of error form. That’s because the \\(\\chi^2\\) isn’t symmetric. But, we now have all the information we need to calculate the confidence interval for the variance in trout length.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA word of warning. This is not a robust procedure. If the assumption of normality is not met, this interval will give poor results. This is not true of the \\(t\\) interval for the mean.\n\n\n3.2.2 Hypothesis Testing\nThe goal of hypothesis tests is to use an observed data set to answer a yes/no question about a characteristic of a larger population from which the observed data set was drawn.\nFor example, let’s consider the ticks again. The entomologist knows from a literature review that the prevalence of the lyme disease pathogen in the black-legged tick is 0.02. They are interested in whether the presence of the pathogen is more prevalent in the new tick species. The yes/no question we will answer is whether the resulting data provide convincing evidence that the pathogen is more prevalent in the new species. These questions lead to two competing claims, both stated in terms of parameters of a probability distribution\n\nNull hypothesis\n\n\n\n\n\n\n\nAlternative hypothesis\n\n\n\n\n\n\nWe will choose between the competing claims by assessing whether the data conflict so much with H\\(_0\\) that the null hypothesis cannot be considered reasonable. If this happens, we’ll reject the notion of H\\(_0\\) and conclude that H\\(_a\\) must be true. We will NEVER conclude that the null hypothesis is true.\nHypothesis tests work by assuming the null hypothesis is true, and assessing the plausibility of the observed data under that assumption.\n\n\n\n\n\nThe entomologist examined 200 ticks randomly selected from the study region. If we assume the null hypothesis is true, then we expect to see\n\n\n\nIn fact, 18/200 ticks were infected with the pathogen. The question then becomes\n\n\n\n\nTo see how unusual this sample result of 18/200 is, we again need the sampling distribution of the sample statistic. As a reminder, the Central Limit Theorem says\n\n\n\n\n\n\n\n\nSo we can use normal distribution to see how unusual 18/200 is, if the null hypothesis is true.\n\n\n\n\n\n\n\n\n\nExample: Let’s consider the potato chip example again. The bag claims that a serving contains 130 calories. We want to test whether this is true. This leads to the hypotheses\n\n\n\n\n\nWhat’s different here?\n\n\n\n\n\n\nWe can again appeal to the Central Limit Theorem and the \\(t\\) distribution to characterize the sampling distribution of \\(\\bar{X}\\), which leads to the test statistic\n\n\n\n\n\nThe random sample of 35 bags had a sample mean of \\(\\bar{x}=134\\) and standard deviation \\(s=17\\).\n\n\n\n\n\n\n\nBut now what is “more unusual” assuming the null hypothesis is true?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen in doubt, use a two-sided test! Use a one-sided test only if you truly have interest in only one direction. Why? To fully answer this, we need to address decision errors.\nAnytime we’re using sample data to make decisions about a larger population we can potentially make a mistake. We can make an incorrect decision in a hypothesis test or calculate a confidence interval that does not capture the true population parameter. In a hypothesis test, there are four possible outcomes at the outset of the study:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType I error:\n\n\n\n\n\n\nType II error:\n\n\n\n\n\nExamples:\n\nDoping in the Olympics\n\n\n\n\n\n\n\n\nCriminal trial\n\n\n\n\n\n\n\n\nDiagnostic test for a serious disease\n\n\n\n\n\n\n\n\n\nErrors require a balancing act. We want to reduce the chance of making a Type I error but this will necessarily increase the chance of making a Type II error. The best we can do is to set the probability of a Type I error. We can do this through setting the significance level.\nSignificance level:\n\n\n\n\n\nSo how does this fit in with one- and two-sided hypotheses?\n\n\n\n\n\n\n\n\n\nHow else can we control Type I error?\n\nSet up tests before seeing the data.\nCollect enough data that the test has sufficient power. Power is the probability of correcting rejecting a false null hypothesis. It’s a function of how big the true difference is (which we don’t know and can’t control), the expected variability in our responses (also can’t control, but might know), and the sample size (which we can control). We’ll talk more about power later on in the semester.\n\nThe two examples we’ve seen have both utilized a test statistic with the form\n\n\n\n\n\n\n\nWith confidence intervals, we mentioned that many confidence intervals have the form estimate \\(\\pm\\) margin of error, but not all do. We saw an example, a confidence interval for a variance, that had a different form. Similarly, many tests have a test statistic of the form\n\\[\n\\frac{\\hbox{estimate - hypothesized value}}{\\hbox{standard error of estimate}}\n\\]\nbut not all do.\nExample: The Poisson distribution is often a good model for scenarios in which we are counting occurrences over some specified time or space unit. However, the Poisson distribution has the characteristic that the population mean = population variance. In some scenarios, this may not be true, invalidating the Poisson as a possible model. We can use hypothesis testing to determine if the Poisson is a reaonable model for a data set. A scientist is interested in modeling the number of parasites found on a host, and believes the Poisson may be a feasible model.\n\n\n\n\n\nThe researcher examines 80 host organisms, and records the number of parasites found on each. The data are:\n\n\n\nNumber of Parasites\n0\n1\n2\n3\n4\n5\n\n\n\n\nNumber of hosts\n20\n28\n19\n9\n3\n1\n\n\n\nThere is not a single mean or proportion (or variance) we can calculate here that will summarize how closely these data follow a Poisson distribution. Instead, we’ll need to come up with a new test statistic.\nThe first thing we’ll need is an estimate of the Poisson parameter, \\(\\lambda\\).\n\n\n\n\nNow, if we consider the Poisson distribution with \\(\\lambda = 1.375\\) we can calculate some probabilities:\n\n\n\nX\nProbability\n\n\n\n\n0\n0.2528\n\n\n1\n0.3477\n\n\n2\n0.2390\n\n\n3\n0.1095\n\n\n4\n0.0377\n\n\n5\n0.0104\n\n\nover 5\n0.0029\n\n\n\nIf the Poisson distribution is a realistic model, we would expect to see our data fall into these categories in about these proportions. So, we expect\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Parasites\n0\n1\n2\n3\n4\n5\n&gt;5\n\n\n\n\nNumber of hosts\n20\n28\n19\n9\n3\n1\n0\n\n\nExpected\n20.224\n27.816\n19.12\n8.76\n3.016\n0.832\n0.232\n\n\n\nand we can compare the observed counts to the expected counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Parasites\n0\n1\n2\n3\n4\n5\n&gt;5\n\n\n\n\nNumber of hosts\n20\n28\n19\n9\n3\n1\n0\n\n\nExpected\n20.224\n27.816\n19.12\n8.76\n3.016\n0.832\n0.232\n\n\nDifference\n-0.224\n0.184\n-0.12\n0.24\n-0.016\n0.168\n-0.232\n\n\n\nBut we’ve got another problem.\n\n\n\n\nAgain, our solution will be squaring! This time we’ll also scale. The resulting test statistic is:\n\n\n\n\n\n\nThis is called the chi-squared goodness-of-fit test. Under the null hypothesis, this test statistic will follow a \\(\\chi^2\\) distribution with \\(k-1\\) degrees of freedom, where \\(k\\) is the number of categories. However, we also need a big enough sample so that all expected counts are at least 5. That’s not true here. What now?\n\n\n\n\n\n\n\n\n\nNumber of Parasites\n0\n1\n2\n\\(\\geq\\) 3\n\n\n\n\nNumber of hosts\n20\n28\n19\n13\n\n\nExpected\n20.224\n27.816\n19.12\n12.84\n\n\nDifference\n-0.224\n0.184\n-0.12\n0.16\n\n\n\nSo now,\n\n\n\n\n\n\n\nWe can also easily do this in R:\n\nhost&lt;-c(20,28,19,9, 3, 1, 0)\nchisq.test(host,p=c(0.2528, 0.3477, 0.2390, 0.1095, 0.0377, 0.0104, 0.0029))\n\nWarning in chisq.test(host, p = c(0.2528, 0.3477, 0.239, 0.1095, 0.0377, :\nChi-squared approximation may be incorrect\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  host\nX-squared = 0.27703, df = 6, p-value = 0.9996\n\n\nSo R is telling us our sample size isn’t big enough for the \\(\\chi^2\\) distribution to work. Like we did by hand, we can collapse some categories.\n\nhost&lt;-c(20,28,19,13)\nchisq.test(host,p=c(0.2528, 0.3477, 0.2390, 0.1605))\n\n\n    Chi-squared test for given probabilities\n\ndata:  host\nX-squared = 0.0064451, df = 3, p-value = 0.9999\n\n\nSo it appears we have no reason to doubt that the Poisson distribution is a good model for these data.\nNow that we’ve seen the logic behind statistical inference, we can move on to more complicated situations. We’ll consider cases in which we have a single explanatory variable and a single response variable. We’ll first cover the case where the explanatory variable is categorical with only two levels, and the response variable is either categorical or numeric (comparing two groups). We’ll then move on to the case where the the explanatory variable is categorical with more than two levels, and the response variable is categorical or numeric. Finally, we’ll consider the case where the explanatory and response variable are both numeric.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling Distributions and Foundations of Statistical Inference</span>"
    ]
  },
  {
    "objectID": "Section 4 One Pred Two Levels.html",
    "href": "Section 4 One Pred Two Levels.html",
    "title": "4  One Predictor/Explanatory Variable, Two Levels",
    "section": "",
    "text": "4.1 Categorical Response, Two Levels\nAs mentioned at the end of Chapter 3, we’ll now move on to cases in which we have a single explanatory variable and a single response variable. In this section, we’ll cover the case where the explanatory variable is categorical with only two levels, and the response variable is either categorical or numeric. This means that in this chapter, we’ll be focusing on comparing two groups.\nData like these may show up in a spreadsheet like\nFirst, we’ll consider situations in which two categorical variables are measured on each unit in the sample, and each variable has two possible values. In cases like these, typically one variable is considered the response and one variable is considered explanatory. The explanatory variable may be randomly assigned (like whether a subject was assigned to a treatment or control) or it may be merely observed (like smoking status).\nThe two possible values of the explanatory variable lead to two groups, and we’re interested in comparing the population proportions that arise from these two groups. We’ll focus on the function of parameters \\(p_1 - p_2\\). The natural estimate of this is \\(\\hat{p_1} - \\hat{p_2}\\): the difference in the sample proportions. We’ll be constructing hypothesis tests to compare \\(p_1\\) to \\(p_2\\) and finding confidence intervals to estimate \\(p_1 - p_2\\). To demonstrate these methods, we’ll use an example.\nExample: Biologists studying crows will capture a crow, tag it, and release it. These crows seem to remember the scientists who caught them and will scold them later. A study to examine this effect had several scientists wear a caveman mask while they trapped and tagged 7 crows. A control group did not tag any crows and wore a different mask. The two masks did not elicit different reactions from the crows before tagging. Volunteers then strolled around town wearing one or the other of the two masks.The crows scolded a person wearing a caveman mask in 158 out of 444 encounters with crows, whereas crows scolded a person in a neutral mask in 109 out of 922 encounters. Suppose we want to find a confidence interval for the difference in proportion of crow scoldings between volunteers wearing the caveman mask and those wearing the neutral mask.\nFor a single proportion, we needed two conditions to be met to ensure the sampling distribution of \\(\\hat{p}\\) is approximately normal:\nIf these conditions are met, then\nWe must meet similar conditions to ensure the sampling distribution of \\(\\hat p_1 - \\hat p_2\\) is approximately normal:\nIf these conditions are met, then\nLike before we don’t know \\(p_1\\) and \\(p_2\\), so we’ll use our best guess. And, like before, our best guess will change depending on whether we’re constructing a confidence interval or carrying out a hypothesis test.\nHow is this going to play out in a confidence interval?\nLet’s go back to the crows.\nHow is this going to play out in a hypothesis test?\nAgain, let’s go back to the crows.\nWe can also do this is R or SAS, but either program will use a different (but also not really) approach. We’ll start with R.\nprop.test(x=c(158,109), n=c(444,922))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(158, 109) out of c(444, 922)\nX-squared = 106.11, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.1867976 0.2884716\nsample estimates:\n   prop 1    prop 2 \n0.3558559 0.1182213\nFrom this output, what looks familiar?\nWhat doesn’t look familiar?\nBut is this what we actually tested?\nprop.test(x=c(158,109), n=c(444,922),alternative=\"greater\",correct=FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(158, 109) out of c(444, 922)\nX-squared = 107.62, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.196371 1.000000\nsample estimates:\n   prop 1    prop 2 \n0.3558559 0.1182213\nWhat do you notice?\nIn SAS, we can use proc freq. First, we’ll need to read in the data.\nHere’s the data set\nNow, to get the test\nwhich gives\nWhy do both R and SAS carry out the procedure like this? This is a method that can be used in situations where both the explanatory and response variable have any number (\\(\\geq 2\\)) possible values. We’ll see examples like this in the next chapter.\nBut! The methods are actually doing the same thing. Let’s look at the test we carried out by hand.\nExample: Do metal tags on penguins harm them? Scientists trying to tell penguins apart have several ways to tag the birds. One method involves wrapping metal strips with ID numbers around the penguin’s flipper, while another involves electronic tags. Neither tag seems to physically harm the penguins. However, since tagged penguins are used to study all penguins, scientists wanted to determine whether the tagging method has any effect. Data were collected over a 10-year time span from a sample of 100 penguins that were randomly given either metal or electronic tags. Information collected includes number of chicks, survival over the decade, and length of time on foraging trips. Let’s first consider survival. We’re interested in estimating the difference in survival rate between penguins with metal tags and penguins with electronic tags.\nWhat parameters are of interest here?\nWhat kind of research question are we trying to answer? What does this imply about the analysis method?\nWhat next?\nLet’s do the analysis in R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One Predictor/Explanatory Variable, Two Levels</span>"
    ]
  },
  {
    "objectID": "Section 4 One Pred Two Levels.html#categorical-response-two-levels",
    "href": "Section 4 One Pred Two Levels.html#categorical-response-two-levels",
    "title": "4  One Predictor/Explanatory Variable, Two Levels",
    "section": "",
    "text": "data crows;\n  input mask $ NumScold Total;\n  response=\"Scold\"; Count=NumScold;  output;\n  response=\"NoScold\"; Count=Total-NumScold; output;\n  datalines;\nCaveman 158 444\nNeutral 109 922\n;\n\nproc print data=crows; run;\n\n                                         Num\n                      Obs     mask      Scold    Total    response    Count\n\n                       1     Caveman     158      444      Scold       158\n                       2     Caveman     158      444      NoSco       286\n                       3     Neutral     109      922      Scold       109\n                       4     Neutral     109      922      NoSco       813\n\nproc freq data=crows;\n  weight Count;\n  table mask*response/chisq;\nrun;\n\n\n\n\nOutput from proc freq",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One Predictor/Explanatory Variable, Two Levels</span>"
    ]
  },
  {
    "objectID": "Section 4 One Pred Two Levels.html#quantitative-response",
    "href": "Section 4 One Pred Two Levels.html#quantitative-response",
    "title": "4  One Predictor/Explanatory Variable, Two Levels",
    "section": "4.2 Quantitative Response",
    "text": "4.2 Quantitative Response\nExample: Data were collected over a 10-year time span from a sample of 100 penguins that were randomly given either metal or electronic tags. Information collected includes number of chicks, survival over the decade, and length of time on foraging trips. Now let’s focus on length of foraging trips. Longer foraging trips can jeopardize both breeding success and survival of chicks waiting for food. Suppose we’re interested in estimating the difference in mean trip length between penguins with metal tags and those with electronic tags.\nWhat are the parameters?\n\n\n\n\nWhat kind of research question are we trying to answer? What does this imply about the analysis method?\n\n\n\n\n\n\nWhat’s different from the crows example?\n\n\n\n\n\nThis means we will have to change our analysis approach.\nJust like with the \\(t\\) methods for single means, we need to check conditions to determine whether we can the \\(t\\)-distribution to construct tests and form confidence intervals for the difference in means.\n\nIndependence–both between and within groups\nCheck normality of each group separately (basically checking for extreme outliers)\nIf these are both met, then the standard error of \\(\\bar x_1 - \\bar x_2\\) is \\(SE = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma^2_2}{n_2}}\\) with \\(df=\\) really complicated (you’ll see we get non-integers in R/SAS–it’s doing the complicated calculation). We’ll use \\(\\min(n_1-1, n_2-1)\\) if we’re not using R/SAS. We won’t know \\(\\sigma^2_1\\) and \\(\\sigma_2^2\\), so we’ll approximate the standard error using \\(SE \\approx \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}}\\)\n\n\n\n\n\n\nAs with tests for a single mean (and one proportion, and two proportions), our test statistic will have the usual form: \\[\n\\hbox{test statistic} = \\frac{\\hbox{observed value - hypothesized value}}{SE}\n\\] In the case of two means, this is \\[\nT = \\frac{(\\bar x_1 - \\bar x_2) - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\] When the null hypothesis is true and the conditions are met, \\(T\\) has a \\(t\\)-distribution with \\(df=\\min(n_1-1,n_2-1)\\).\nConfidence intervals will also have the same form: \\[\n\\hbox{observed statistic} \\pm \\hbox{multiplier} \\times SE\n\\] For this specific situation of comparing two independent means, this is \\[\n(\\bar x_1 - \\bar x_2) \\pm t^*_{df} \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}}\n\\] and we’ll again use \\(df=\\min(n_1-1,n_2-1)\\) (or let R/SAS calculate it for us).\nWith two proportions, our SE changed depending on whether we were doing a hypothesis test or calculating a confidence interval. Here, it doesn’t. Any guesses why?\n\n\n\n\nExample: There were 344 foraging trips made by penguins with a metal tag, and those trips had a sample mean of \\(\\bar x_{M} =12.70\\) days with standard deviation \\(s_{M}=3.71\\) days. For those penguins with electronic tags, the mean was \\(\\bar x_{E} = 11.60\\) days with standard deviation \\(s_{E}=4.53\\) days over 512 trips.\n\n\n\n\n\n\n\n\n\n\nExample: Another variable measured was the date penguins arrive at the breeding site, with later arrivals hurting breeding success. Arrival date is measured as the number of days after 1 November. The researchers are interested in whether metal tagged penguins arrive later than electronic tagged penguins.\nWhat are the parameters?\n\n\n\n\nWhat kind of research question are we trying to answer? What does this imply about the analysis method?\n\n\n\n\n\n\n\n\nMean arrival date for the 167 times metal tagged penguins arrived was 7 December (37 days after 1 November) with standard deviation \\(s_{M}=38.77\\) days, while mean arrival date for the 189 times electronic tagged penguins arrived was 21 November (21 days after 1 November) with standard deviation \\(s_{E}=27.50\\)\n\n\n\n\n\n\n\n\n\n\nWe can easily carry out \\(t\\) tests and confidence intervals in R and SAS. But, we can’t for the penguin data. R and SAS both require the whole data set, as opposed to summary statistics.\nExample: The data set may be found in Canvas: ‘NutritionStudy.csv’. This data set gives nutrition levels in people’s blood as well as information about their eating habits, and comes from a random sample of 315 US adults. Suppose we are interested in estimating the difference in mean beta carotene blood level between smokers and non-smokers. Let’s start by reading the data into R.\n\nNutritionStudy&lt;-read.csv(\"NutritionStudy.csv\",header=TRUE)\n\nhead(NutritionStudy)\n\n  ID Age Smoke Quetelet Vitamin Calories  Fat Fiber Alcohol Cholesterol\n1  1  64    No  21.4838       1   1298.8 57.0   6.3     0.0       170.3\n2  2  76    No  23.8763       1   1032.5 50.1  15.8     0.0        75.8\n3  3  38    No  20.0108       2   2372.3 83.6  19.1    14.1       257.9\n4  4  40    No  25.1406       3   2449.5 97.5  26.5     0.5       332.6\n5  5  72    No  20.9850       1   1952.1 82.6  16.2     0.0       170.8\n6  6  40    No  27.5214       3   1366.9 56.0   9.6     1.3       154.6\n  BetaDiet RetinolDiet BetaPlasma RetinolPlasma    Sex VitaminUse PriorSmoke\n1     1945         890        200           915 Female    Regular          2\n2     2653         451        124           727 Female    Regular          1\n3     6321         660        328           721 Female Occasional          2\n4     1061         864        153           615 Female         No          2\n5     2863        1209         92           799 Female    Regular          1\n6     1729        1439        148           654 Female         No          2\n\n\nIf I wanted to calculate the confidence interval by hand, I could use R to get the summary statistics\n\nNutMeanNS&lt;-mean(NutritionStudy$BetaPlasma[NutritionStudy$Smoke==\"No\"])\nNutMeanNS\n\n[1] 200.7316\n\nNutSDNS&lt;-sd(NutritionStudy$BetaPlasma[NutritionStudy$Smoke==\"No\"])\nNutSDNS\n\n[1] 192.2929\n\nsize_NS&lt;-sum(with(data=NutritionStudy, Smoke==\"No\"))\nsize_NS\n\n[1] 272\n\nNutMeanS&lt;-mean(NutritionStudy$BetaPlasma[NutritionStudy$Smoke==\"Yes\"])\nNutMeanS\n\n[1] 121.3256\n\nNutSDS&lt;-sd(NutritionStudy$BetaPlasma[NutritionStudy$Smoke==\"Yes\"])\nNutSDS\n\n[1] 78.81163\n\nsize_S&lt;-sum(with(data=NutritionStudy, Smoke==\"Yes\"))\nsize_S\n\n[1] 43\n\n\nSo now we have all the components we need to calculate the confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also let R calculate the confidence interval for us, using t.test:\n\nt.test(BetaPlasma~Smoke,data=NutritionStudy)\n\n\n    Welch Two Sample t-test\n\ndata:  BetaPlasma by Smoke\nt = 4.7421, df = 139.15, p-value = 5.175e-06\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n  46.29873 112.51335\nsample estimates:\n mean in group No mean in group Yes \n         200.7316          121.3256 \n\n\nWe can change the confidence level easily\n\nt.test(BetaPlasma~Smoke,data=NutritionStudy, conf.level=0.90)\n\n\n    Welch Two Sample t-test\n\ndata:  BetaPlasma by Smoke\nt = 4.7421, df = 139.15, p-value = 5.175e-06\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n90 percent confidence interval:\n  51.67854 107.13353\nsample estimates:\n mean in group No mean in group Yes \n         200.7316          121.3256 \n\n\nLet’s carry out a test by hand, to see how it compares to the output.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One Predictor/Explanatory Variable, Two Levels</span>"
    ]
  },
  {
    "objectID": "Section 4 One Pred Two Levels.html#comparing-paired-means",
    "href": "Section 4 One Pred Two Levels.html#comparing-paired-means",
    "title": "4  One Predictor/Explanatory Variable, Two Levels",
    "section": "4.3 Comparing Paired Means",
    "text": "4.3 Comparing Paired Means\nEverything we’ve done so far has assumed independence among observations. If we only had one group, it was just independence among observations. If we had two or more groups, it was independence between and within groups. Now, we’ll turn our attention to a common situation: dependence between groups. Specifically, a particular dependency–pairing. This occurs in before/after studies, other studies in which subjects are matched. For example, considering the price of a item purchased from two different retailers.\nIn these situations, we generally take the difference between the two values, and consider the difference as our observation. So, for example, if we want to compare cost of textbooks between the campus bookstore and Amazon, we’d randomly select a set of book titles, and find their price at both the bookstore and Amazon. We’d find the difference in price, and use those differences as our observations.\nNote that we’re distinguishing between and .\nGood news: we’ve already seen how to construct tests and confidence intervals here! We just use the same techniques we used for a single mean (Chapter 3), but on the differences. The changes come in the form of the hypotheses and interpretation of the confidence interval.\nExample: Long distance runners contend that moderate exposure to ozone increases lung capacity. In investigate this possibility, a researcher exposed 12 rats to ozone at the rate of 2 ppm for a period of 30 days. The lung capacity of the rats was determined at the beginning of the study and again after 30 days of ozone exposure. The lung capacities (in mL) are in the file ‘ozone.csv’.\n\nozone&lt;-read.csv(\"ozone.csv\",header=TRUE)\n\nhead(ozone)\n\n  Rat Before After\n1   1    8.7   9.4\n2   2    7.9   9.8\n3   3    8.3   9.9\n4   4    8.4  10.3\n5   5    9.2   8.9\n6   6    9.1   8.8\n\n\nThe first thing we’ll do is calculate the change in lung capacity.\n\nozone$diff&lt;-ozone$Before - ozone$After\n\nhead(ozone)\n\n  Rat Before After diff\n1   1    8.7   9.4 -0.7\n2   2    7.9   9.8 -1.9\n3   3    8.3   9.9 -1.6\n4   4    8.4  10.3 -1.9\n5   5    9.2   8.9  0.3\n6   6    9.1   8.8  0.3\n\n\nWhat is the parameter?\n\n\n\n\nWhat research question are we trying to answer?\n\n\n\n\nWhat does this imply about the analysis method we should use?\n\n\n\n\n\n\n\nt.test(ozone$diff)\n\n\n    One Sample t-test\n\ndata:  ozone$diff\nt = -3.885, df = 11, p-value = 0.002541\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.8928932 -0.5237735\nsample estimates:\nmean of x \n-1.208333 \n\n\n\n\nt.test(ozone$Before,ozone$After,paired=TRUE)\n\n\n    Paired t-test\n\ndata:  ozone$Before and ozone$After\nt = -3.885, df = 11, p-value = 0.002541\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.8928932 -0.5237735\nsample estimates:\nmean difference \n      -1.208333 \n\n\nWhat is incorrect about this analysis in R? How can we fix it?\n\n\n\n\n\n\nt.test(ozone$Before,ozone$After,paired=TRUE,alternative=\"less\")\n\n\n    Paired t-test\n\ndata:  ozone$Before and ozone$After\nt = -3.885, df = 11, p-value = 0.001271\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n       -Inf -0.6497695\nsample estimates:\nmean difference \n      -1.208333 \n\n\nLet’s write a couple of conclusions here.\n\nWhat are the consequences of ignoring pairing? Let’s look at a different example.\nExample: Suppose you are playing baseball and hit a hard line drive. You want to turn a single into a double. Does the path you take to round first base make a difference? A masters thesis way back in 1970 considered the difference between a “narrow angle” and a “wide angle” around first base. Suppose we have 22 baseball players who have volunteered to participate. There are a couple ways we could design an experiment to see if there is a difference.\n\nRandomly assign 11 players to run a wide angle and 11 players to run a narrow angle. Problems: some players may be faster than others. Ideally, randomization will equally distribute the speedy runners between the two groups, but there is no guarantee. Speed could be a confounding variable.\nHave each of the 22 runners run both angles, with the angle run first randomized using a coin. This allows each player to serve as their own control.\n\nThe second option is what the thesis writer did–he randomly determined the angle the player would take first. He then used a stopwatch the time the run from going from a spot 35 feet past home to a spot 15 feet before 2nd base. After a rest period, the runner then ran the second angle. This controls for runner-to-runner variability. It’s important to randomize the order of the treatments, where possible! (This isn’t possible in before-and-after type studies.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.075\n\n\nParameter of interest:\nHypotheses of interest:\n\n\n\n\n\nObserved statistic:\nLike before, we’re trying to determine if it’s surprising to see such a large difference as \\(\\bar x_d = 0.075\\) just by chance, if running strategy has no effect on running time.\n\nt.test(bases$diff)\n\n\n    One Sample t-test\n\ndata:  bases$diff\nt = 3.9837, df = 21, p-value = 0.0006754\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.03584814 0.11415186\nsample estimates:\nmean of x \n    0.075 \n\nt.test(time~angle,data=bases2)\n\n\n    Welch Two Sample t-test\n\ndata:  time by angle\nt = 0.93383, df = 41.899, p-value = 0.3557\nalternative hypothesis: true difference in means between group narrow and group wide is not equal to 0\n95 percent confidence interval:\n -0.08709334  0.23709334\nsample estimates:\nmean in group narrow   mean in group wide \n            5.534091             5.459091",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One Predictor/Explanatory Variable, Two Levels</span>"
    ]
  },
  {
    "objectID": "Section 4 One Pred Two Levels.html#comparing-variances",
    "href": "Section 4 One Pred Two Levels.html#comparing-variances",
    "title": "4  One Predictor/Explanatory Variable, Two Levels",
    "section": "4.4 Comparing Variances",
    "text": "4.4 Comparing Variances\nOften it is useful to test if variances from independent populations are different. For example,\n\na geneticist wants to test equality of the genotypic variances of kernel weight of two different corn populations\nan engineer is interested in comparing the process variance of two different types of production systems used to make a electronic component\nthe two-sample \\(t\\)-test is based on the assumption that the variances of the two populations are equal\n\nAssume the data from both populations follow a normal distribution with different means and possibly different variances. We want to test\n\n\n\n\n\nA natural approach would be to take samples of \\(n_1\\) and \\(n_2\\) observations from the two populations, and compute \\(s_1^2\\) and \\(s_2^2\\). We could then take the ratio \\(s_1^2/s_2^2\\) and reject H\\(_0\\) if the ratio is very different from 1. But, we need to know the sampling distribution of the ratio \\(S^2_1/S^2_2\\). Recall\n\n\n\n\n\n\nSir R. A. Fisher showed that the ratio of two independent \\(\\chi^2\\) distributions has an \\(F\\) distribution with \\((n_1-1)\\) and \\((n_2-1)\\) degrees of freedom. Specifically,\n\n\n\n\n\nUnder H\\(_0: \\sigma^2_1 = \\sigma^2_2\\) then\n\n\n\nThe \\(F\\) distribution\n\nis non-negative, unimodal, and right skewed\n\n\n\n\n\n\n\n\n\n\n\nthe shape of the distribution depends on the numerator and denominator degrees of freedom\n\nSo, to test H\\(_0: \\sigma_1^2 = \\sigma_2^2\\) versus H\\(_a: \\sigma_1^2 \\neq \\sigma_2^2\\), we can\n\nAssume that \\(S_1^2\\) is the larger of the two sample variaces\nUse \\(S_1^2/S_2^2\\) as a test statistic. Under H\\(_0\\), this ratio will follow an \\(F\\) distribution with \\(n_1-1\\) and \\(n_2-1\\) degrees of freedom\nUse the \\(F\\) distribution to see if \\(s_1^2/s_2^2\\) is enough bigger than 1 to convince us the null hypothesis is not true (always a right-tail test!)\n\nExample: The writings of different authors can be partially characterized by the variablity in the lengths of their sentences. Two manuscripts, \\(A\\) and \\(B\\), are found by a historian and they want to know whether they have the same author. Fifteen sentences from each are chosen at random, and word counts per sentence are recorded. The historian finds \\(s_A^2= 0.114\\) and \\(s_B^2= 0.143\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use var.test() in R, but must have the whole data set.\nExample: Earlier, we used a data set with nutrition levels in people’s blood as well as information about their eating habits that came from a random sample of 315 US adults.\n\nNutritionStudy&lt;-read.csv(\"NutritionStudy.csv\",header=TRUE)\n\nhead(NutritionStudy)\n\n  ID Age Smoke Quetelet Vitamin Calories  Fat Fiber Alcohol Cholesterol\n1  1  64    No  21.4838       1   1298.8 57.0   6.3     0.0       170.3\n2  2  76    No  23.8763       1   1032.5 50.1  15.8     0.0        75.8\n3  3  38    No  20.0108       2   2372.3 83.6  19.1    14.1       257.9\n4  4  40    No  25.1406       3   2449.5 97.5  26.5     0.5       332.6\n5  5  72    No  20.9850       1   1952.1 82.6  16.2     0.0       170.8\n6  6  40    No  27.5214       3   1366.9 56.0   9.6     1.3       154.6\n  BetaDiet RetinolDiet BetaPlasma RetinolPlasma    Sex VitaminUse PriorSmoke\n1     1945         890        200           915 Female    Regular          2\n2     2653         451        124           727 Female    Regular          1\n3     6321         660        328           721 Female Occasional          2\n4     1061         864        153           615 Female         No          2\n5     2863        1209         92           799 Female    Regular          1\n6     1729        1439        148           654 Female         No          2\n\n\nThe Quetelet index is a measure of body mass (BMI). Suppose we are interested in whether smokers and nonsmokers have the same variability of BMI scores.\n\n\n\n\n\n\nvar.test(Quetelet~Smoke,data=NutritionStudy)\n\n\n    F test to compare two variances\n\ndata:  Quetelet by Smoke\nF = 1.563, num df = 271, denom df = 42, p-value = 0.08157\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.9438906 2.3908039\nsample estimates:\nratio of variances \n          1.563047",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>One Predictor/Explanatory Variable, Two Levels</span>"
    ]
  }
]